{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import splitfolders\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from pipeline_torch_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=yaml.load(open('config.yml', 'r'), Loader=yaml.FullLoader)\n",
    "seed = config['model_config']['initial_seed']\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True\n",
    "LVL_all = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 928 files [00:18, 49.15 files/s]\n"
     ]
    }
   ],
   "source": [
    "if LVL_all == True:\n",
    "    # define training and test data directories\n",
    "    data_dir  = r'C:\\Users\\FINAELB\\Documents\\Aalto\\Dayyan\\TL\\Data\\Scenario 05'\n",
    "    train_valid_dir = os.path.join(data_dir) \n",
    "    #test_dir  = os.path.join(data_dir, 'test')\n",
    "\n",
    "    splitfolders.ratio(input=train_valid_dir, output='split_data', ratio=(0.6, 0.4))\n",
    "    train_dir='split_data/train'\n",
    "    valid_dir='split_data/val'\n",
    "\n",
    "    splitfolders.ratio(input='split_data/val', output='Valid_Test', ratio=(0.5, 0.5))\n",
    "    valid_dir='Valid_Test/train'\n",
    "    test_dir='Valid_Test/val'\n",
    "\n",
    "    # Selecting mean and std values according to ImageNet dataset\n",
    "    mean = torch.tensor( [0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "else: \n",
    "    # define training and test data directories# define training and test data directories\n",
    "    data_dir  = r'C:\\Users\\FINAELB\\Documents\\Aalto\\Dayyan\\TL\\Data\\Scenario 06\\01 Training'\n",
    "    data_dir_test  = r'C:\\Users\\FINAELB\\Documents\\Aalto\\Dayyan\\TL\\Data\\Scenario 06\\02 Testing'\n",
    "    train_valid_dir = os.path.join(data_dir) \n",
    "    test_dir  = os.path.join(data_dir, 'test')\n",
    "\n",
    "    splitfolders.ratio(input=train_valid_dir, output='split_data', ratio=(0.6, 0.4))\n",
    "    train_dir='split_data/train'\n",
    "    valid_dir='split_data/val'\n",
    "\n",
    "    test_dir=r'C:\\Users\\FINAELB\\Documents\\Aalto\\Dayyan\\TL\\Data\\Scenario 06\\02 Testing'\n",
    "\n",
    "    # Selecting mean and std values according to ImageNet dataset\n",
    "    mean = torch.tensor( [0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and transform data using ImageFolder\n",
    "data_transforms = {\n",
    "    'train':  transforms.Compose([\n",
    "                                transforms.Resize([224,224]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean,std)\n",
    "                                ]),\n",
    "    'validation':  transforms.Compose([\n",
    "                                transforms.Resize([224,224]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean,std)\n",
    "                                ]),\n",
    "    'test':  transforms.Compose([\n",
    "                                transforms.Resize([224,224]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean,std)\n",
    "                                ])\n",
    "}\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=data_transforms[\"train\"])\n",
    "valid_data = datasets.ImageFolder(valid_dir, transform=data_transforms[\"validation\"])\n",
    "test_data  = datasets.ImageFolder(test_dir, transform=data_transforms[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models and prepare for TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FINAELB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\FINAELB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## MODEL 1: Resnet 18\n",
    "model_1 = models.resnet18(pretrained=True)\n",
    "layers=list(model_1._modules.keys())\n",
    "\n",
    "layers_frozen=layers[0:8]\n",
    "\n",
    "for layer in layers_frozen:\n",
    "    for param in model_1._modules[layer].parameters():\n",
    "        param.requires_grad=False\n",
    "        \n",
    "# modify last layer to match it our classes\n",
    "n_inputs = model_1.fc.in_features\n",
    "last_layer = nn.Linear(n_inputs, len(train_data.classes))\n",
    "model_1.fc = last_layer\n",
    "\n",
    "model_1 = models.resnet18(pretrained=True)\n",
    "layers=list(model_1._modules.keys())\n",
    "\n",
    "layers_frozen=layers[0:8]\n",
    "\n",
    "for layer in layers_frozen:\n",
    "    for param in model_1._modules[layer].parameters():\n",
    "        param.requires_grad=False\n",
    "        \n",
    "# modify last layer to match it our classes\n",
    "n_inputs = model_1.fc.in_features\n",
    "last_layer = nn.Linear(n_inputs, len(train_data.classes))\n",
    "model_1.fc = last_layer\n",
    "\n",
    "model_1 = model_1.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FINAELB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## MODEL 2: Resnet 50\n",
    "model_2 = models.resnet50(pretrained=True)\n",
    "layers=list(model_2._modules.keys())\n",
    "\n",
    "layers_frozen=layers[0:8]\n",
    "\n",
    "for layer in layers_frozen:\n",
    "    for param in model_2._modules[layer].parameters():\n",
    "        param.requires_grad=False\n",
    "        \n",
    "# modify last layer to match it our classes\n",
    "n_inputs = model_2.fc.in_features\n",
    "last_layer = nn.Linear(n_inputs, len(train_data.classes))\n",
    "model_2.fc = last_layer\n",
    "\n",
    "model_2 = model_2.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FINAELB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## MODEL 3: VGG 16\n",
    "\n",
    "model_3 = models.vgg16(pretrained=True)\n",
    "layers=list(model_3._modules.keys())\n",
    "\n",
    "layers_frozen=layers[0:30]\n",
    "\n",
    "for layer in layers_frozen:\n",
    "    for param in model_3._modules[layer].parameters():\n",
    "        param.requires_grad=False\n",
    "\n",
    "# modify last layer to match it our classes\n",
    "n_inputs = model_3.classifier[6].in_features\n",
    "last_layer = nn.Linear(n_inputs, len(train_data.classes))\n",
    "model_3.classifier[6] = last_layer\n",
    "\n",
    "model_3 = model_3.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FINAELB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# MODEL 4: AlexNet\n",
    "\n",
    "model_4 = models.alexnet(pretrained=True)\n",
    "layers=list(model_4._modules.keys())\n",
    "\n",
    "layers_frozen=layers[0:12]\n",
    "\n",
    "for layer in layers_frozen:\n",
    "    for param in model_4._modules[layer].parameters():\n",
    "        param.requires_grad=False\n",
    "\n",
    "# modify last layer to match it our classes\n",
    "n_inputs = model_4.classifier[6].in_features\n",
    "last_layer = nn.Linear(n_inputs, len(train_data.classes))\n",
    "model_4.classifier[6] = last_layer\n",
    "\n",
    "model_4 = model_4.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FINAELB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# MODEL 5: GoogleNet\n",
    "\n",
    "model_5 = models.googlenet(pretrained=True)\n",
    "layers=list(model_5._modules.keys())\n",
    "\n",
    "layers_frozen=layers[0:16]\n",
    "\n",
    "for layer in layers_frozen:\n",
    "    for param in model_5._modules[layer].parameters():\n",
    "        param.requires_grad=False\n",
    "\n",
    "# modify last layer to match it our classes\n",
    "\n",
    "n_inputs = model_5.fc.in_features\n",
    "last_layer = nn.Linear(n_inputs, len(train_data.classes))\n",
    "model_5.fc = last_layer\n",
    "\n",
    "model_5 = model_5.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FINAELB\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# MODEL 6: Inception\n",
    "model_6 = models.inception_v3(pretrained=True)\n",
    "layers=list(model_6._modules.keys())\n",
    "\n",
    "layers_frozen=layers[0:19]\n",
    "\n",
    "for layer in layers_frozen:\n",
    "    for param in model_6._modules[layer].parameters():\n",
    "        param.requires_grad\n",
    "\n",
    "# modify last layer to match it our classes\n",
    "n_inputs = model_6.fc.in_features\n",
    "last_layer = nn.Linear(n_inputs, len(train_data.classes))\n",
    "model_6.fc = last_layer\n",
    "\n",
    "model_6 = model_6.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_model_file(old_name, new_name, directory='models'):\n",
    "    # Construct full file paths\n",
    "    old_file_path = os.path.join(directory, old_name)\n",
    "    new_file_path = os.path.join(directory, new_name)\n",
    "    \n",
    "    # Check if the old file exists\n",
    "    if os.path.exists(old_file_path):\n",
    "        # Rename the file\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "        print(f\"File renamed from {old_name} to {new_name}\")\n",
    "    else:\n",
    "        print(f\"File {old_name} does not exist in the directory {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = PipelineTorch(model_1, config)\n",
    "if skip_training == False:\n",
    "    pipeline1.train(train_data, valid_data, config['model_config']['version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File model_v7.pth does not exist in the directory models\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "old_file_name = 'model_' + config['model_config']['version'] + '.pth'\n",
    "new_file_name = 'model1_' + config['model_config']['version'] + '.pth'\n",
    "rename_model_file(old_file_name, new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = PipelineTorch(model_2, config)\n",
    "if skip_training == False: \n",
    "    pipeline2.train(train_data, valid_data, config['model_config']['version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File model_v7.pth does not exist in the directory models\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "old_file_name = 'model_' + config['model_config']['version'] + '.pth'\n",
    "new_file_name = 'model2_' + config['model_config']['version'] + '.pth'\n",
    "rename_model_file(old_file_name, new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline3 = PipelineTorch(model_3, config)\n",
    "if skip_training == False: \n",
    "    pipeline3.train(train_data, valid_data, config['model_config']['version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File model_v7.pth does not exist in the directory models\n"
     ]
    }
   ],
   "source": [
    "old_file_name = 'model_' + config['model_config']['version'] + '.pth'\n",
    "new_file_name = 'model3_' + config['model_config']['version'] + '.pth'\n",
    "rename_model_file(old_file_name, new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline4 = PipelineTorch(model_4, config)\n",
    "if skip_training == False: \n",
    "    pipeline4.train(train_data, valid_data, config['model_config']['version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File model_v7.pth does not exist in the directory models\n"
     ]
    }
   ],
   "source": [
    "old_file_name = 'model_' + config['model_config']['version'] + '.pth'\n",
    "new_file_name = 'model4_' + config['model_config']['version'] + '.pth'\n",
    "rename_model_file(old_file_name, new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline5 = PipelineTorch(model_5, config)\n",
    "if skip_training == False: \n",
    "    pipeline5.train(train_data, valid_data, config['model_config']['version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File model_v7.pth does not exist in the directory models\n"
     ]
    }
   ],
   "source": [
    "old_file_name = 'model_' + config['model_config']['version'] + '.pth'\n",
    "new_file_name = 'model5_' + config['model_config']['version'] + '.pth'\n",
    "rename_model_file(old_file_name, new_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post_Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'models\\model1_v7.pth'\n",
      "=> loaded checkpoint 'models\\model1_v7.pth' (epoch 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FINAELB\\Documents\\Aalto\\Dayyan\\TL\\TL_CWT\\DL_classification\\pipeline_torch_models.py:151: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint=torch.load(os.path.join(self.artifact_path, f\"model{nb_model}_{version}.pth\"),map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'models\\model2_v7.pth'\n",
      "=> loaded checkpoint 'models\\model2_v7.pth' (epoch 25)\n",
      "=> loading checkpoint 'models\\model3_v7.pth'\n",
      "=> loaded checkpoint 'models\\model3_v7.pth' (epoch 25)\n",
      "=> loading checkpoint 'models\\model4_v7.pth'\n",
      "=> loaded checkpoint 'models\\model4_v7.pth' (epoch 25)\n",
      "=> loading checkpoint 'models\\model5_v7.pth'\n",
      "=> loaded checkpoint 'models\\model5_v7.pth' (epoch 24)\n"
     ]
    }
   ],
   "source": [
    "pipeline1.load_checkpoint(config['model_config']['version'],nb_model=1)\n",
    "pipeline2.load_checkpoint(config['model_config']['version'],nb_model=2)\n",
    "pipeline3.load_checkpoint(config['model_config']['version'],nb_model=3)\n",
    "pipeline4.load_checkpoint(config['model_config']['version'],nb_model=4)\n",
    "pipeline5.load_checkpoint(config['model_config']['version'],nb_model=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses1_test, average_loss1_test, predictions1_test, real_labels1_test, acc1_test= pipeline1.predict(test_data)   \n",
    "losses2_test, average_loss2_test, predictions2_test, real_labels2_test, acc2_test= pipeline2.predict(test_data)   \n",
    "losses3_test, average_loss3_test, predictions3_test, real_labels3_test, acc3_test= pipeline3.predict(test_data)   \n",
    "losses4_test, average_loss4_test, predictions4_test, real_labels4_test, acc4_test= pipeline4.predict(test_data)   \n",
    "losses5_test, average_loss5_test, predictions5_test, real_labels5_test, acc5_test= pipeline5.predict(test_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses1_valid, average_loss1_valid, predictions1_valid, real_labels1_valid, acc1_valid= pipeline1.predict(valid_data)   \n",
    "losses2_valid, average_loss2_valid, predictions2_valid, real_labels2_valid, acc2_valid= pipeline2.predict(valid_data)   \n",
    "losses3_valid, average_loss3_valid, predictions3_valid, real_labels3_valid, acc3_valid= pipeline3.predict(valid_data)   \n",
    "losses4_valid, average_loss4_valid, predictions4_valid, real_labels4_valid, acc4_valid= pipeline4.predict(valid_data)   \n",
    "losses5_valid, average_loss5_valid, predictions5_valid, real_labels5_valid, acc5_valid= pipeline5.predict(valid_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "losses1_train, average_loss1_train, predictions1_train, real_labels1_train, acc1_train= pipeline1.predict(train_data)   \n",
    "losses2_train, average_loss2_train, predictions2_train, real_labels2_train, acc2_train= pipeline2.predict(train_data)   \n",
    "losses3_train, average_loss3_train, predictions3_train, real_labels3_train, acc3_train= pipeline3.predict(train_data)   \n",
    "losses4_train, average_loss4_train, predictions4_train, real_labels4_train, acc4_train= pipeline4.predict(train_data)   \n",
    "losses5_train, average_loss5_train, predictions5_train, real_labels5_train, acc5_train= pipeline5.predict(train_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data from the pipeline predictions (replace these with your actual outputs)\n",
    "train_accuracies = [acc1_train[0], acc2_train[0], acc3_train[0], acc4_train[0], acc5_train[0]]\n",
    "valid_accuracies = [acc1_valid[0], acc2_valid[0], acc3_valid[0], acc4_valid[0], acc5_valid[0]]\n",
    "test_accuracies = [acc1_test[0], acc2_test[0], acc3_test[0], acc4_test[0], acc5_test[0]]\n",
    "\n",
    "train_losses = [average_loss1_train, average_loss2_train, average_loss3_train, average_loss4_train, average_loss5_train]\n",
    "valid_losses = [average_loss1_valid, average_loss2_valid, average_loss3_valid, average_loss4_valid, average_loss5_valid]\n",
    "test_losses = [average_loss1_test, average_loss2_test, average_loss3_test, average_loss4_test, average_loss5_test]\n",
    "\n",
    "# Creating a DataFrame with the organized data\n",
    "df = pd.DataFrame({\n",
    "    'Model': ['Model 1', 'Model 2', 'Model 3', 'Model 4', 'Model 5'],\n",
    "    'Train Accuracy': train_accuracies,\n",
    "    'Validation Accuracy': valid_accuracies,\n",
    "    'Test Accuracy': test_accuracies,\n",
    "    'Train Loss': train_losses,\n",
    "    'Validation Loss': valid_losses,\n",
    "    'Test Loss': test_losses\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Test Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model 1</td>\n",
       "      <td>98.362069</td>\n",
       "      <td>97.185430</td>\n",
       "      <td>93.965517</td>\n",
       "      <td>0.075259</td>\n",
       "      <td>0.084471</td>\n",
       "      <td>0.300255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model 2</td>\n",
       "      <td>98.103448</td>\n",
       "      <td>96.854305</td>\n",
       "      <td>93.103448</td>\n",
       "      <td>0.070188</td>\n",
       "      <td>0.084305</td>\n",
       "      <td>0.268473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model 3</td>\n",
       "      <td>97.586207</td>\n",
       "      <td>96.192053</td>\n",
       "      <td>90.948276</td>\n",
       "      <td>0.131648</td>\n",
       "      <td>0.142863</td>\n",
       "      <td>0.417479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model 4</td>\n",
       "      <td>92.327586</td>\n",
       "      <td>85.761589</td>\n",
       "      <td>64.655172</td>\n",
       "      <td>0.071190</td>\n",
       "      <td>0.080288</td>\n",
       "      <td>1.568532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model 5</td>\n",
       "      <td>94.051724</td>\n",
       "      <td>89.072848</td>\n",
       "      <td>73.706897</td>\n",
       "      <td>0.116101</td>\n",
       "      <td>0.146076</td>\n",
       "      <td>0.670321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model  Train Accuracy  Validation Accuracy  Test Accuracy  Train Loss  \\\n",
       "0  Model 1       98.362069            97.185430      93.965517    0.075259   \n",
       "1  Model 2       98.103448            96.854305      93.103448    0.070188   \n",
       "2  Model 3       97.586207            96.192053      90.948276    0.131648   \n",
       "3  Model 4       92.327586            85.761589      64.655172    0.071190   \n",
       "4  Model 5       94.051724            89.072848      73.706897    0.116101   \n",
       "\n",
       "   Validation Loss  Test Loss  \n",
       "0         0.084471   0.300255  \n",
       "1         0.084305   0.268473  \n",
       "2         0.142863   0.417479  \n",
       "3         0.080288   1.568532  \n",
       "4         0.146076   0.670321  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Drive_Health_App",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
