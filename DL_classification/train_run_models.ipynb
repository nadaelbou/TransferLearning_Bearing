{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import splitfolders\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "from pipeline_torch_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=yaml.load(open('config.yml', 'r'), Loader=yaml.FullLoader)\n",
    "seed = config['model_config']['initial_seed']\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False\n",
    "LVL_all = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LVL_all == True:\n",
    "    # define training and test data directories\n",
    "    data_dir  = r''\n",
    "    train_valid_dir = os.path.join(data_dir) \n",
    "    #test_dir  = os.path.join(data_dir, 'test')\n",
    "\n",
    "    splitfolders.ratio(input=train_valid_dir, output='split_data', ratio=(0.6, 0.4))\n",
    "    train_dir='split_data/train'\n",
    "    valid_dir='split_data/val'\n",
    "\n",
    "    splitfolders.ratio(input='split_data/val', output='Valid_Test', ratio=(0.5, 0.5))\n",
    "    valid_dir='Valid_Test/train'\n",
    "    test_dir='Valid_Test/val'\n",
    "\n",
    "    # Selecting mean and std values according to ImageNet dataset\n",
    "    mean = torch.tensor( [0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "else: \n",
    "    # define training and test data directories# define training and test data directories\n",
    "    data_dir  = r''\n",
    "    data_dir_test  = r''\n",
    "    train_valid_dir = os.path.join(data_dir) \n",
    "    test_dir  = os.path.join(data_dir, 'test')\n",
    "\n",
    "    splitfolders.ratio(input=train_valid_dir, output='split_data', ratio=(0.6, 0.4))\n",
    "    train_dir='split_data/train'\n",
    "    valid_dir='split_data/val'\n",
    "\n",
    "    test_dir=r''\n",
    "\n",
    "    # Selecting mean and std values according to ImageNet dataset\n",
    "    mean = torch.tensor( [0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and transform data using ImageFolder\n",
    "data_transforms = {\n",
    "    'train':  transforms.Compose([\n",
    "                                transforms.Resize([224,224]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean,std)\n",
    "                                ]),\n",
    "    'validation':  transforms.Compose([\n",
    "                                transforms.Resize([224,224]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean,std)\n",
    "                                ]),\n",
    "    'test':  transforms.Compose([\n",
    "                                transforms.Resize([224,224]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean,std)\n",
    "                                ])\n",
    "}\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=data_transforms[\"train\"])\n",
    "valid_data = datasets.ImageFolder(valid_dir, transform=data_transforms[\"validation\"])\n",
    "test_data  = datasets.ImageFolder(test_dir, transform=data_transforms[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models and prepare for TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL 1: Resnet 18\n",
    "model_1 = models.resnet18(pretrained=True)\n",
    "layers=list(model_1._modules.keys())\n",
    "\n",
    "layers_frozen=layers[0:8]\n",
    "\n",
    "for layer in layers_frozen:\n",
    "    for param in model_1._modules[layer].parameters():\n",
    "        param.requires_grad=False\n",
    "        \n",
    "# modify last layer to match it our classes\n",
    "n_inputs = model_1.fc.in_features\n",
    "last_layer = nn.Linear(n_inputs, len(train_data.classes))\n",
    "model_1.fc = last_layer\n",
    "\n",
    "model_1 = models.resnet18(pretrained=True)\n",
    "layers=list(model_1._modules.keys())\n",
    "\n",
    "layers_frozen=layers[0:8]\n",
    "\n",
    "for layer in layers_frozen:\n",
    "    for param in model_1._modules[layer].parameters():\n",
    "        param.requires_grad=False\n",
    "        \n",
    "# modify last layer to match it our classes\n",
    "n_inputs = model_1.fc.in_features\n",
    "last_layer = nn.Linear(n_inputs, len(train_data.classes))\n",
    "model_1.fc = last_layer\n",
    "\n",
    "model_1 = model_1.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL 2: Resnet 50\n",
    "model_2 = models.resnet50(pretrained=True)\n",
    "layers=list(model_2._modules.keys())\n",
    "\n",
    "layers_frozen=layers[0:8]\n",
    "\n",
    "for layer in layers_frozen:\n",
    "    for param in model_2._modules[layer].parameters():\n",
    "        param.requires_grad=False\n",
    "        \n",
    "# modify last layer to match it our classes\n",
    "n_inputs = model_2.fc.in_features\n",
    "last_layer = nn.Linear(n_inputs, len(train_data.classes))\n",
    "model_2.fc = last_layer\n",
    "\n",
    "model_2 = model_2.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL 3: VGG 16\n",
    "\n",
    "model_3 = models.vgg16(pretrained=True)\n",
    "layers=list(model_3._modules.keys())\n",
    "\n",
    "layers_frozen=layers[0:30]\n",
    "\n",
    "for layer in layers_frozen:\n",
    "    for param in model_3._modules[layer].parameters():\n",
    "        param.requires_grad=False\n",
    "\n",
    "# modify last layer to match it our classes\n",
    "n_inputs = model_3.classifier[6].in_features\n",
    "last_layer = nn.Linear(n_inputs, len(train_data.classes))\n",
    "model_3.classifier[6] = last_layer\n",
    "\n",
    "model_3 = model_3.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 4: AlexNet\n",
    "\n",
    "model_4 = models.alexnet(pretrained=True)\n",
    "layers=list(model_4._modules.keys())\n",
    "\n",
    "layers_frozen=layers[0:12]\n",
    "\n",
    "for layer in layers_frozen:\n",
    "    for param in model_4._modules[layer].parameters():\n",
    "        param.requires_grad=False\n",
    "\n",
    "# modify last layer to match it our classes\n",
    "n_inputs = model_4.classifier[6].in_features\n",
    "last_layer = nn.Linear(n_inputs, len(train_data.classes))\n",
    "model_4.classifier[6] = last_layer\n",
    "\n",
    "model_4 = model_4.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 5: GoogleNet\n",
    "\n",
    "model_5 = models.googlenet(pretrained=True)\n",
    "layers=list(model_5._modules.keys())\n",
    "\n",
    "layers_frozen=layers[0:16]\n",
    "\n",
    "for layer in layers_frozen:\n",
    "    for param in model_5._modules[layer].parameters():\n",
    "        param.requires_grad=False\n",
    "\n",
    "# modify last layer to match it our classes\n",
    "\n",
    "n_inputs = model_5.fc.in_features\n",
    "last_layer = nn.Linear(n_inputs, len(train_data.classes))\n",
    "model_5.fc = last_layer\n",
    "\n",
    "model_5 = model_5.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_model_file(old_name, new_name, directory='models'):\n",
    "    # Construct full file paths\n",
    "    old_file_path = os.path.join(directory, old_name)\n",
    "    new_file_path = os.path.join(directory, new_name)\n",
    "    \n",
    "    # Check if the old file exists\n",
    "    if os.path.exists(old_file_path):\n",
    "        # Rename the file\n",
    "        os.rename(old_file_path, new_file_path)\n",
    "        print(f\"File renamed from {old_name} to {new_name}\")\n",
    "    else:\n",
    "        print(f\"File {old_name} does not exist in the directory {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = PipelineTorch(model_1, config)\n",
    "if skip_training == False:\n",
    "    pipeline1.train(train_data, valid_data, config['model_config']['version'])\n",
    "    # Example usage\n",
    "    old_file_name = 'model_' + config['model_config']['version'] + '.pth'\n",
    "    new_file_name = 'model1_' + config['model_config']['version'] + '.pth'\n",
    "    rename_model_file(old_file_name, new_file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = PipelineTorch(model_2, config)\n",
    "if skip_training == False: \n",
    "    pipeline2.train(train_data, valid_data, config['model_config']['version'])\n",
    "    # Example usage\n",
    "    old_file_name = 'model_' + config['model_config']['version'] + '.pth'\n",
    "    new_file_name = 'model2_' + config['model_config']['version'] + '.pth'\n",
    "    rename_model_file(old_file_name, new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline3 = PipelineTorch(model_3, config)\n",
    "if skip_training == False: \n",
    "    pipeline3.train(train_data, valid_data, config['model_config']['version'])\n",
    "    old_file_name = 'model_' + config['model_config']['version'] + '.pth'\n",
    "    new_file_name = 'model3_' + config['model_config']['version'] + '.pth'\n",
    "    rename_model_file(old_file_name, new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline4 = PipelineTorch(model_4, config)\n",
    "if skip_training == False: \n",
    "    pipeline4.train(train_data, valid_data, config['model_config']['version'])\n",
    "    old_file_name = 'model_' + config['model_config']['version'] + '.pth'\n",
    "    new_file_name = 'model4_' + config['model_config']['version'] + '.pth'\n",
    "    rename_model_file(old_file_name, new_file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline5 = PipelineTorch(model_5, config)\n",
    "if skip_training == False: \n",
    "    pipeline5.train(train_data, valid_data, config['model_config']['version'])\n",
    "    old_file_name = 'model_' + config['model_config']['version'] + '.pth'\n",
    "    new_file_name = 'model5_' + config['model_config']['version'] + '.pth'\n",
    "    rename_model_file(old_file_name, new_file_name)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post_Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1.load_checkpoint(config['model_config']['version'],nb_model=1)\n",
    "pipeline2.load_checkpoint(config['model_config']['version'],nb_model=2)\n",
    "pipeline3.load_checkpoint(config['model_config']['version'],nb_model=3)\n",
    "pipeline4.load_checkpoint(config['model_config']['version'],nb_model=4)\n",
    "pipeline5.load_checkpoint(config['model_config']['version'],nb_model=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses1_test, average_loss1_test, predictions1_test, real_labels1_test, acc1_test= pipeline1.predict(test_data)   \n",
    "losses2_test, average_loss2_test, predictions2_test, real_labels2_test, acc2_test= pipeline2.predict(test_data)   \n",
    "losses3_test, average_loss3_test, predictions3_test, real_labels3_test, acc3_test= pipeline3.predict(test_data)   \n",
    "losses4_test, average_loss4_test, predictions4_test, real_labels4_test, acc4_test= pipeline4.predict(test_data)   \n",
    "losses5_test, average_loss5_test, predictions5_test, real_labels5_test, acc5_test= pipeline5.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses1_valid, average_loss1_valid, predictions1_valid, real_labels1_valid, acc1_valid= pipeline1.predict(valid_data)   \n",
    "losses2_valid, average_loss2_valid, predictions2_valid, real_labels2_valid, acc2_valid= pipeline2.predict(valid_data)   \n",
    "losses3_valid, average_loss3_valid, predictions3_valid, real_labels3_valid, acc3_valid= pipeline3.predict(valid_data)   \n",
    "losses4_valid, average_loss4_valid, predictions4_valid, real_labels4_valid, acc4_valid= pipeline4.predict(valid_data)   \n",
    "losses5_valid, average_loss5_valid, predictions5_valid, real_labels5_valid, acc5_valid= pipeline5.predict(valid_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "losses1_train, average_loss1_train, predictions1_train, real_labels1_train, acc1_train= pipeline1.predict(train_data)   \n",
    "losses2_train, average_loss2_train, predictions2_train, real_labels2_train, acc2_train= pipeline2.predict(train_data)   \n",
    "losses3_train, average_loss3_train, predictions3_train, real_labels3_train, acc3_train= pipeline3.predict(train_data)   \n",
    "losses4_train, average_loss4_train, predictions4_train, real_labels4_train, acc4_train= pipeline4.predict(train_data)   \n",
    "losses5_train, average_loss5_train, predictions5_train, real_labels5_train, acc5_train= pipeline5.predict(train_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from the pipeline predictions (replace these with your actual outputs)\n",
    "train_accuracies = [acc1_train[0], acc2_train[0], acc3_train[0], acc4_train[0], acc5_train[0]]\n",
    "valid_accuracies = [acc1_valid[0], acc2_valid[0], acc3_valid[0], acc4_valid[0], acc5_valid[0]]\n",
    "test_accuracies = [acc1_test[0], acc2_test[0], acc3_test[0], acc4_test[0], acc5_test[0]]\n",
    "\n",
    "train_losses = [average_loss1_train, average_loss2_train, average_loss3_train, average_loss4_train, average_loss5_train]\n",
    "valid_losses = [average_loss1_valid, average_loss2_valid, average_loss3_valid, average_loss4_valid, average_loss5_valid]\n",
    "test_losses = [average_loss1_test, average_loss2_test, average_loss3_test, average_loss4_test, average_loss5_test]\n",
    "\n",
    "# Creating a DataFrame with the organized data\n",
    "df = pd.DataFrame({\n",
    "    'Model': ['Model 1', 'Model 2', 'Model 3', 'Model 4', 'Model 5'],\n",
    "    'Train Accuracy': train_accuracies,\n",
    "    'Validation Accuracy': valid_accuracies,\n",
    "    'Test Accuracy': test_accuracies,\n",
    "    'Train Loss': train_losses,\n",
    "    'Validation Loss': valid_losses,\n",
    "    'Test Loss': test_losses\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Drive_Health_App",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
